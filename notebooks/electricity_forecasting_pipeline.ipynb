{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Electricity Consumption Time Series Forecasting - Complete Pipeline\n",
        "\n",
        "## Project Overview\n",
        "\n",
        "This notebook implements a comprehensive time series forecasting pipeline for predicting household electricity consumption. The project follows industry best practices and includes:\n",
        "\n",
        "1. **Data Loading and Preprocessing**: Handling missing values, resampling to hourly frequency\n",
        "2. **Exploratory Data Analysis**: Time series decomposition, stationarity tests, autocorrelation analysis\n",
        "3. **Feature Engineering**: Temporal features, lag features, rolling statistics\n",
        "4. **Model Development**: Classical (ARIMA, SARIMA, ETS, STL), ML (XGBoost, Random Forest, Linear Regression), and Advanced (LSTM, GRU, Prophet) models\n",
        "5. **Model Evaluation**: Comprehensive metrics and visualizations\n",
        "6. **Model Comparison**: Statistical comparison and best model selection\n",
        "\n",
        "## Objective\n",
        "\n",
        "Forecast the next hour's average Global_active_power consumption using minute-level electricity consumption data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports\n",
        "\n",
        "Import all necessary libraries and set up the environment. We'll use our modular code structure for clean, maintainable code.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard library imports\n",
        "import sys\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Add src to path for imports\n",
        "sys.path.append('../')\n",
        "\n",
        "# Data processing\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Time series analysis\n",
        "from statsmodels.tsa.stattools import adfuller, kpss\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "from scipy import stats\n",
        "\n",
        "# Project modules\n",
        "from src.data.data_loader import load_data\n",
        "from src.data.data_preprocessor import preprocess_data, resample_to_hourly, handle_missing_values, combine_datetime\n",
        "from src.data.data_validator import validate_data\n",
        "from src.features.feature_engineer import create_all_features\n",
        "from src.features.feature_selection import select_features\n",
        "from src.validation.time_series_split import time_series_split\n",
        "from src.models.classical_models import ARIMAModel, SARIMAModel, ETSModel, STLModel\n",
        "from src.models.ml_models import XGBoostModel, RandomForestModel, LinearRegressionModel\n",
        "from src.models.dl_models import LSTMModel, GRUModel, ProphetModel\n",
        "from src.evaluation.metrics import calculate_metrics, print_metrics\n",
        "from src.evaluation.visualizations import plot_forecast, plot_residuals, plot_model_comparison\n",
        "from src.evaluation.model_comparison import compare_models, select_best_model, generate_evaluation_report\n",
        "from src.utils.config import Config\n",
        "from src.utils.logger import setup_logger\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Configuration\n",
        "config = Config()\n",
        "logger = setup_logger(log_file=config.LOG_FILE, log_level=config.LOG_LEVEL)\n",
        "\n",
        "print(\"âœ“ All imports successful!\")\n",
        "print(f\"âœ“ Configuration loaded: Train split = {config.TRAIN_SPLIT*100:.0f}%, Forecast horizon = {config.FORECAST_HORIZON} hours\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Loading\n",
        "\n",
        "Load the household power consumption dataset. The data is in semicolon-separated format with \"?\" representing missing values. We'll parse it properly and handle the date/time columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the data\n",
        "# Note: For faster execution during development, you can use nrows parameter\n",
        "# For full analysis, set nrows=None\n",
        "df_raw = load_data(config.DATA_PATH, nrows=None)\n",
        "\n",
        "print(f\"Data shape: {df_raw.shape}\")\n",
        "print(f\"\\nColumn names: {list(df_raw.columns)}\")\n",
        "print(f\"\\nData types:\\n{df_raw.dtypes}\")\n",
        "print(f\"\\nFirst few rows:\")\n",
        "df_raw.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for missing values and data quality\n",
        "print(\"Missing values per column:\")\n",
        "print(df_raw.isnull().sum())\n",
        "print(f\"\\nTotal missing values: {df_raw.isnull().sum().sum()}\")\n",
        "print(f\"\\nDate range in raw data:\")\n",
        "print(f\"  Date column range: {df_raw['Date'].min()} to {df_raw['Date'].max()}\")\n",
        "print(f\"  Time column range: {df_raw['Time'].min()} to {df_raw['Time'].max()}\")\n",
        "\n",
        "# Basic statistics\n",
        "print(\"\\nBasic statistics:\")\n",
        "df_raw.describe()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Preprocessing\n",
        "\n",
        "This step is critical for time series analysis:\n",
        "1. **Combine Date and Time**: Create a single datetime index\n",
        "2. **Handle Missing Values**: Use interpolation (appropriate for time series)\n",
        "3. **Resample to Hourly**: Aggregate minute-level data to hourly using mean\n",
        "4. **Outlier Treatment**: Detect and cap extreme outliers\n",
        "5. **Data Validation**: Ensure chronological order and data quality\n",
        "\n",
        "**Justification**: Time series models require continuous, evenly-spaced data. Missing values break continuity, and outliers can distort model training. Hourly aggregation reduces noise while preserving important patterns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Complete preprocessing pipeline\n",
        "df_processed = preprocess_data(\n",
        "    df_raw,\n",
        "    date_col='Date',\n",
        "    time_col='Time',\n",
        "    missing_method='interpolate',  # Interpolation preserves temporal patterns\n",
        "    outlier_method='cap',  # Cap outliers rather than remove to preserve data continuity\n",
        "    resample_method='mean',  # Mean aggregation for hourly data\n",
        "    target_col='Global_active_power'\n",
        ")\n",
        "\n",
        "print(f\"Processed data shape: {df_processed.shape}\")\n",
        "print(f\"Date range: {df_processed.index.min()} to {df_processed.index.max()}\")\n",
        "print(f\"Frequency: {pd.infer_freq(df_processed.index)}\")\n",
        "print(f\"\\nMissing values after preprocessing: {df_processed.isnull().sum().sum()}\")\n",
        "\n",
        "# Validate the processed data\n",
        "is_valid, issues = validate_data(df_processed, target_col='Global_active_power')\n",
        "if is_valid:\n",
        "    print(\"\\nâœ“ Data validation passed!\")\n",
        "else:\n",
        "    print(f\"\\nâš  Data validation issues: {issues}\")\n",
        "\n",
        "df_processed.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Exploratory Data Analysis (EDA)\n",
        "\n",
        "Comprehensive EDA helps us understand:\n",
        "- **Trends**: Long-term patterns in consumption\n",
        "- **Seasonality**: Daily, weekly, monthly patterns\n",
        "- **Stationarity**: Whether the series is stationary (required for some models)\n",
        "- **Autocorrelation**: Relationships between past and current values\n",
        "\n",
        "**Justification**: Understanding these characteristics guides model selection. For example, strong seasonality suggests SARIMA or Prophet, while non-stationarity requires differencing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot the time series\n",
        "target_col = 'Global_active_power'\n",
        "y = df_processed[target_col]\n",
        "\n",
        "plt.figure(figsize=(16, 6))\n",
        "plt.plot(y.index, y.values, linewidth=0.5, alpha=0.7)\n",
        "plt.title('Global Active Power Consumption Over Time', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Date', fontsize=12)\n",
        "plt.ylabel('Global Active Power (kW)', fontsize=12)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Summary statistics\n",
        "print(\"Time Series Summary Statistics:\")\n",
        "print(f\"  Mean: {y.mean():.4f} kW\")\n",
        "print(f\"  Std:  {y.std():.4f} kW\")\n",
        "print(f\"  Min:  {y.min():.4f} kW\")\n",
        "print(f\"  Max:  {y.max():.4f} kW\")\n",
        "print(f\"  Total observations: {len(y)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Time series decomposition\n",
        "# Multiplicative or additive - we'll try additive first\n",
        "decomposition = seasonal_decompose(y, model='additive', period=24)  # 24-hour period\n",
        "\n",
        "fig, axes = plt.subplots(4, 1, figsize=(16, 12))\n",
        "decomposition.observed.plot(ax=axes[0], title='Original', color='blue')\n",
        "decomposition.trend.plot(ax=axes[1], title='Trend', color='green')\n",
        "decomposition.seasonal.plot(ax=axes[2], title='Seasonal (24-hour)', color='orange')\n",
        "decomposition.resid.plot(ax=axes[3], title='Residual', color='red')\n",
        "\n",
        "for ax in axes:\n",
        "    ax.set_ylabel('Power (kW)', fontsize=10)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nDecomposition Summary:\")\n",
        "print(f\"  Trend range: {decomposition.trend.min():.2f} to {decomposition.trend.max():.2f}\")\n",
        "print(f\"  Seasonal range: {decomposition.seasonal.min():.2f} to {decomposition.seasonal.max():.2f}\")\n",
        "print(f\"  Residual std: {decomposition.resid.std():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stationarity tests\n",
        "print(\"STATIONARITY TESTS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Augmented Dickey-Fuller test\n",
        "adf_result = adfuller(y.dropna())\n",
        "print(\"\\n1. Augmented Dickey-Fuller (ADF) Test:\")\n",
        "print(f\"   ADF Statistic: {adf_result[0]:.4f}\")\n",
        "print(f\"   p-value: {adf_result[1]:.4f}\")\n",
        "print(f\"   Critical Values:\")\n",
        "for key, value in adf_result[4].items():\n",
        "    print(f\"      {key}: {value:.4f}\")\n",
        "if adf_result[1] <= 0.05:\n",
        "    print(\"   âœ“ Series is STATIONARY (p < 0.05)\")\n",
        "else:\n",
        "    print(\"   âœ— Series is NON-STATIONARY (p >= 0.05)\")\n",
        "    print(\"   â†’ Differencing may be required for ARIMA models\")\n",
        "\n",
        "# KPSS test\n",
        "kpss_result = kpss(y.dropna(), regression='c')\n",
        "print(\"\\n2. KPSS Test:\")\n",
        "print(f\"   KPSS Statistic: {kpss_result[0]:.4f}\")\n",
        "print(f\"   p-value: {kpss_result[1]:.4f}\")\n",
        "print(f\"   Critical Values:\")\n",
        "for key, value in kpss_result[3].items():\n",
        "    print(f\"      {key}: {value:.4f}\")\n",
        "if kpss_result[1] >= 0.05:\n",
        "    print(\"   âœ“ Series is STATIONARY (p >= 0.05)\")\n",
        "else:\n",
        "    print(\"   âœ— Series is NON-STATIONARY (p < 0.05)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Autocorrelation analysis\n",
        "fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
        "\n",
        "# ACF plot\n",
        "plot_acf(y.dropna(), lags=48, ax=axes[0], title='Autocorrelation Function (ACF)', alpha=0.05)\n",
        "axes[0].set_xlabel('Lag (hours)', fontsize=11)\n",
        "axes[0].set_ylabel('ACF', fontsize=11)\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# PACF plot\n",
        "plot_pacf(y.dropna(), lags=48, ax=axes[1], title='Partial Autocorrelation Function (PACF)', alpha=0.05, method='ywm')\n",
        "axes[1].set_xlabel('Lag (hours)', fontsize=11)\n",
        "axes[1].set_ylabel('PACF', fontsize=11)\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nACF/PACF Analysis:\")\n",
        "print(\"  - Significant lags in ACF indicate moving average (MA) components\")\n",
        "print(\"  - Significant lags in PACF indicate autoregressive (AR) components\")\n",
        "print(\"  - Strong seasonal patterns (24h, 168h) suggest SARIMA models\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Seasonal patterns analysis\n",
        "df_processed['hour'] = df_processed.index.hour\n",
        "df_processed['day_of_week'] = df_processed.index.dayofweek\n",
        "df_processed['month'] = df_processed.index.month\n",
        "\n",
        "# Hourly pattern\n",
        "hourly_avg = df_processed.groupby('hour')[target_col].mean()\n",
        "# Day of week pattern\n",
        "dow_avg = df_processed.groupby('day_of_week')[target_col].mean()\n",
        "# Monthly pattern\n",
        "monthly_avg = df_processed.groupby('month')[target_col].mean()\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "hourly_avg.plot(kind='bar', ax=axes[0], color='steelblue', edgecolor='black')\n",
        "axes[0].set_title('Average Consumption by Hour of Day', fontweight='bold')\n",
        "axes[0].set_xlabel('Hour', fontsize=11)\n",
        "axes[0].set_ylabel('Average Power (kW)', fontsize=11)\n",
        "axes[0].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "dow_avg.plot(kind='bar', ax=axes[1], color='coral', edgecolor='black')\n",
        "axes[1].set_title('Average Consumption by Day of Week', fontweight='bold')\n",
        "axes[1].set_xlabel('Day of Week (0=Mon, 6=Sun)', fontsize=11)\n",
        "axes[1].set_ylabel('Average Power (kW)', fontsize=11)\n",
        "axes[1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "monthly_avg.plot(kind='bar', ax=axes[2], color='green', edgecolor='black')\n",
        "axes[2].set_title('Average Consumption by Month', fontweight='bold')\n",
        "axes[2].set_xlabel('Month', fontsize=11)\n",
        "axes[2].set_ylabel('Average Power (kW)', fontsize=11)\n",
        "axes[2].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nSeasonal Patterns:\")\n",
        "print(f\"  Hourly range: {hourly_avg.min():.2f} to {hourly_avg.max():.2f} kW\")\n",
        "print(f\"  Day of week range: {dow_avg.min():.2f} to {dow_avg.max():.2f} kW\")\n",
        "print(f\"  Monthly range: {monthly_avg.min():.2f} to {monthly_avg.max():.2f} kW\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create all features\n",
        "df_features = create_all_features(\n",
        "    df_processed,\n",
        "    target_col=target_col,\n",
        "    lags=config.LAG_FEATURES,\n",
        "    rolling_windows=config.ROLLING_WINDOWS\n",
        ")\n",
        "\n",
        "print(f\"Original features: {len(df_processed.columns)}\")\n",
        "print(f\"Total features after engineering: {len(df_features.columns)}\")\n",
        "print(f\"\\nNew feature categories:\")\n",
        "print(f\"  Temporal features: {len([c for c in df_features.columns if any(x in c for x in ['hour', 'day', 'month', 'year', 'week', 'sin', 'cos'])])}\")\n",
        "print(f\"  Lag features: {len([c for c in df_features.columns if 'lag' in c])}\")\n",
        "print(f\"  Rolling features: {len([c for c in df_features.columns if 'rolling' in c or 'ewm' in c])}\")\n",
        "print(f\"  Derived features: {len([c for c in df_features.columns if any(x in c for x in ['diff', 'change', 'power_factor', 'apparent'])])}\")\n",
        "\n",
        "# Remove temporary columns added during EDA\n",
        "if 'hour' in df_features.columns:\n",
        "    df_features = df_features.drop(columns=['hour', 'day_of_week', 'month'])\n",
        "\n",
        "df_features.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select features for ML models\n",
        "# Remove rows with NaN from lag/rolling features (first few rows)\n",
        "df_features = df_features.dropna()\n",
        "\n",
        "# Select features (exclude target)\n",
        "feature_cols = select_features(\n",
        "    df_features,\n",
        "    target_col=target_col,\n",
        "    method='correlation',\n",
        "    threshold=0.05  # Include features with at least 5% correlation\n",
        ")\n",
        "\n",
        "print(f\"Selected {len(feature_cols)} features for ML models\")\n",
        "print(f\"\\nTop 10 features by correlation:\")\n",
        "feature_corr = df_features[feature_cols + [target_col]].corr()[target_col].abs().sort_values(ascending=False)\n",
        "print(feature_corr.head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Train-Test Split\n",
        "\n",
        "**Critical**: Time series data must be split chronologically, not randomly. Future data should never be used to predict the past.\n",
        "\n",
        "**Justification**: Random splits cause data leakage and unrealistic performance estimates. Chronological splits simulate real-world forecasting where we predict future using only past data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data chronologically\n",
        "train_df, test_df = time_series_split(\n",
        "    df_features,\n",
        "    train_size=config.TRAIN_SPLIT\n",
        ")\n",
        "\n",
        "# Separate target and features\n",
        "y_train = train_df[target_col]\n",
        "y_test = test_df[target_col]\n",
        "\n",
        "# For classical models (work with series only)\n",
        "y_train_series = y_train.copy()\n",
        "y_test_series = y_test.copy()\n",
        "\n",
        "# For ML models (need features)\n",
        "X_train = train_df[feature_cols]\n",
        "X_test = test_df[feature_cols]\n",
        "\n",
        "print(f\"Training set: {len(train_df)} samples ({train_df.index.min()} to {train_df.index.max()})\")\n",
        "print(f\"Test set: {len(test_df)} samples ({test_df.index.min()} to {test_df.index.max()})\")\n",
        "print(f\"\\nTraining target statistics:\")\n",
        "print(f\"  Mean: {y_train.mean():.4f} kW\")\n",
        "print(f\"  Std:  {y_train.std():.4f} kW\")\n",
        "print(f\"\\nTest target statistics:\")\n",
        "print(f\"  Mean: {y_test.mean():.4f} kW\")\n",
        "print(f\"  Std:  {y_test.std():.4f} kW\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Model Training and Evaluation\n",
        "\n",
        "We'll train multiple model types and compare their performance:\n",
        "\n",
        "1. **Classical Models**: ARIMA, SARIMA, ETS, STL\n",
        "2. **Machine Learning Models**: XGBoost, Random Forest, Linear Regression\n",
        "3. **Advanced Models**: LSTM, GRU, Prophet\n",
        "\n",
        "**Justification**: Different models capture different patterns. Classical models are interpretable and work well with strong seasonality. ML models can capture complex non-linear patterns. Deep learning models can learn long-term dependencies.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Store all model results\n",
        "all_metrics = {}\n",
        "all_predictions = {}\n",
        "\n",
        "# Forecast horizon\n",
        "forecast_horizon = config.FORECAST_HORIZON\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.1 Classical Time Series Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ARIMA Model\n",
        "print(\"Training ARIMA model...\")\n",
        "try:\n",
        "    arima_model = ARIMAModel(use_auto=True)\n",
        "    arima_model.fit(y_train_series)\n",
        "    arima_pred = arima_model.predict(n_periods=len(y_test))\n",
        "    arima_metrics = calculate_metrics(y_test.values, arima_pred)\n",
        "    all_metrics['ARIMA'] = arima_metrics\n",
        "    all_predictions['ARIMA'] = arima_pred\n",
        "    print_metrics(arima_metrics, \"ARIMA\")\n",
        "except Exception as e:\n",
        "    print(f\"ARIMA model failed: {e}\")\n",
        "    all_metrics['ARIMA'] = {}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SARIMA Model (with 24-hour seasonality)\n",
        "print(\"Training SARIMA model...\")\n",
        "try:\n",
        "    sarima_model = SARIMAModel(\n",
        "        order=(1, 1, 1),\n",
        "        seasonal_order=(1, 1, 1, 24),  # 24-hour seasonality\n",
        "        use_auto=True\n",
        "    )\n",
        "    sarima_model.fit(y_train_series)\n",
        "    sarima_pred = sarima_model.predict(n_periods=len(y_test))\n",
        "    sarima_metrics = calculate_metrics(y_test.values, sarima_pred)\n",
        "    all_metrics['SARIMA'] = sarima_metrics\n",
        "    all_predictions['SARIMA'] = sarima_pred\n",
        "    print_metrics(sarima_metrics, \"SARIMA\")\n",
        "except Exception as e:\n",
        "    print(f\"SARIMA model failed: {e}\")\n",
        "    all_metrics['SARIMA'] = {}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ETS Model (Exponential Smoothing)\n",
        "print(\"Training ETS model...\")\n",
        "try:\n",
        "    ets_model = ETSModel(trend='add', seasonal='add', seasonal_periods=24)\n",
        "    ets_model.fit(y_train_series)\n",
        "    ets_pred = ets_model.predict(n_periods=len(y_test))\n",
        "    ets_metrics = calculate_metrics(y_test.values, ets_pred)\n",
        "    all_metrics['ETS'] = ets_metrics\n",
        "    all_predictions['ETS'] = ets_pred\n",
        "    print_metrics(ets_metrics, \"ETS\")\n",
        "except Exception as e:\n",
        "    print(f\"ETS model failed: {e}\")\n",
        "    all_metrics['ETS'] = {}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# STL Decomposition Model\n",
        "print(\"Training STL model...\")\n",
        "try:\n",
        "    stl_model = STLModel(period=24, robust=True)\n",
        "    stl_model.fit(y_train_series)\n",
        "    stl_pred = stl_model.predict(n_periods=len(y_test))\n",
        "    stl_metrics = calculate_metrics(y_test.values, stl_pred)\n",
        "    all_metrics['STL'] = stl_metrics\n",
        "    all_predictions['STL'] = stl_pred\n",
        "    print_metrics(stl_metrics, \"STL\")\n",
        "except Exception as e:\n",
        "    print(f\"STL model failed: {e}\")\n",
        "    all_metrics['STL'] = {}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.2 Machine Learning Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# XGBoost Model\n",
        "print(\"Training XGBoost model...\")\n",
        "try:\n",
        "    xgb_model = XGBoostModel(n_estimators=100, max_depth=6, learning_rate=0.1)\n",
        "    xgb_model.fit(X_train, y_train, tune_hyperparameters=False)\n",
        "    xgb_pred = xgb_model.predict(X_test)\n",
        "    xgb_metrics = calculate_metrics(y_test.values, xgb_pred)\n",
        "    all_metrics['XGBoost'] = xgb_metrics\n",
        "    all_predictions['XGBoost'] = xgb_pred\n",
        "    print_metrics(xgb_metrics, \"XGBoost\")\n",
        "    \n",
        "    # Feature importance\n",
        "    print(\"\\nTop 10 Most Important Features:\")\n",
        "    importance = xgb_model.get_feature_importance().head(10)\n",
        "    print(importance)\n",
        "except Exception as e:\n",
        "    print(f\"XGBoost model failed: {e}\")\n",
        "    all_metrics['XGBoost'] = {}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Random Forest Model\n",
        "print(\"Training Random Forest model...\")\n",
        "try:\n",
        "    rf_model = RandomForestModel(n_estimators=100, max_depth=10)\n",
        "    rf_model.fit(X_train, y_train, tune_hyperparameters=False)\n",
        "    rf_pred = rf_model.predict(X_test)\n",
        "    rf_metrics = calculate_metrics(y_test.values, rf_pred)\n",
        "    all_metrics['Random Forest'] = rf_metrics\n",
        "    all_predictions['Random Forest'] = rf_pred\n",
        "    print_metrics(rf_metrics, \"Random Forest\")\n",
        "except Exception as e:\n",
        "    print(f\"Random Forest model failed: {e}\")\n",
        "    all_metrics['Random Forest'] = {}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Linear Regression (Ridge)\n",
        "print(\"Training Linear Regression (Ridge) model...\")\n",
        "try:\n",
        "    lr_model = LinearRegressionModel(model_type='ridge', alpha=1.0)\n",
        "    lr_model.fit(X_train, y_train, tune_hyperparameters=False)\n",
        "    lr_pred = lr_model.predict(X_test)\n",
        "    lr_metrics = calculate_metrics(y_test.values, lr_pred)\n",
        "    all_metrics['Linear Regression'] = lr_metrics\n",
        "    all_predictions['Linear Regression'] = lr_pred\n",
        "    print_metrics(lr_metrics, \"Linear Regression\")\n",
        "except Exception as e:\n",
        "    print(f\"Linear Regression model failed: {e}\")\n",
        "    all_metrics['Linear Regression'] = {}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.3 Advanced Models\n",
        "\n",
        "**Note**: Deep learning models (LSTM/GRU) and Prophet may take longer to train. They are optional but provide state-of-the-art performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prophet Model (Facebook's time series model)\n",
        "print(\"Training Prophet model...\")\n",
        "try:\n",
        "    prophet_model = ProphetModel(daily_seasonality=True, weekly_seasonality=True, yearly_seasonality=True)\n",
        "    prophet_model.fit(y_train_series)\n",
        "    prophet_forecast = prophet_model.predict(n_periods=len(y_test), freq='H')\n",
        "    prophet_pred = prophet_model.get_forecast_values(prophet_forecast)\n",
        "    prophet_metrics = calculate_metrics(y_test.values, prophet_pred)\n",
        "    all_metrics['Prophet'] = prophet_metrics\n",
        "    all_predictions['Prophet'] = prophet_pred\n",
        "    print_metrics(prophet_metrics, \"Prophet\")\n",
        "except Exception as e:\n",
        "    print(f\"Prophet model failed: {e}\")\n",
        "    all_metrics['Prophet'] = {}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Model Comparison and Visualization\n",
        "\n",
        "Compare all models and visualize their performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comparison DataFrame\n",
        "comparison_df = compare_models(all_metrics, primary_metric='RMSE')\n",
        "print(\"\\nModel Comparison (sorted by RMSE):\")\n",
        "print(comparison_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize model comparison\n",
        "plot_model_comparison(all_metrics, metric_name='RMSE', save_path='outputs/model_comparison_rmse.png')\n",
        "plot_model_comparison(all_metrics, metric_name='MAE', save_path='outputs/model_comparison_mae.png')\n",
        "plot_model_comparison(all_metrics, metric_name='MAPE', save_path='outputs/model_comparison_mape.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select best model\n",
        "best_model_name, best_model_metrics = select_best_model(all_metrics, primary_metric='RMSE')\n",
        "print(f\"\\nðŸ† Best Model: {best_model_name}\")\n",
        "print(\"\\nBest Model Metrics:\")\n",
        "print_metrics(best_model_metrics, best_model_name)\n",
        "\n",
        "# Plot forecasts for best model\n",
        "best_pred = all_predictions[best_model_name]\n",
        "plot_forecast(\n",
        "    y_test,\n",
        "    best_pred,\n",
        "    title=f'Best Model Forecast: {best_model_name}',\n",
        "    save_path=f'outputs/best_model_forecast_{best_model_name.replace(\" \", \"_\")}.png'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot residuals for best model\n",
        "plot_residuals(\n",
        "    y_test.values,\n",
        "    best_pred,\n",
        "    title=f'Residual Analysis: {best_model_name}',\n",
        "    save_path=f'outputs/residuals_{best_model_name.replace(\" \", \"_\")}.png'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Final Forecast and Summary\n",
        "\n",
        "Generate final forecast for the next 24-48 hours and create evaluation report.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate final forecast for next 24-48 hours using best model\n",
        "# Use full dataset for training\n",
        "y_full = df_features[target_col]\n",
        "\n",
        "print(f\"Generating {config.FORECAST_HORIZON} hour ahead forecast using {best_model_name}...\")\n",
        "\n",
        "# Retrain best model on full data\n",
        "if best_model_name in ['ARIMA', 'SARIMA', 'ETS', 'STL', 'Prophet']:\n",
        "    # Classical models\n",
        "    if best_model_name == 'ARIMA':\n",
        "        final_model = ARIMAModel(use_auto=True)\n",
        "        final_model.fit(y_full)\n",
        "        final_forecast = final_model.predict(n_periods=config.FORECAST_HORIZON)\n",
        "    elif best_model_name == 'SARIMA':\n",
        "        final_model = SARIMAModel(use_auto=True)\n",
        "        final_model.fit(y_full)\n",
        "        final_forecast = final_model.predict(n_periods=config.FORECAST_HORIZON)\n",
        "    elif best_model_name == 'ETS':\n",
        "        final_model = ETSModel(trend='add', seasonal='add', seasonal_periods=24)\n",
        "        final_model.fit(y_full)\n",
        "        final_forecast = final_model.predict(n_periods=config.FORECAST_HORIZON)\n",
        "    elif best_model_name == 'STL':\n",
        "        final_model = STLModel(period=24, robust=True)\n",
        "        final_model.fit(y_full)\n",
        "        final_forecast = final_model.predict(n_periods=config.FORECAST_HORIZON)\n",
        "    elif best_model_name == 'Prophet':\n",
        "        final_model = ProphetModel()\n",
        "        final_model.fit(y_full)\n",
        "        forecast_df = final_model.predict(n_periods=config.FORECAST_HORIZON, freq='H')\n",
        "        final_forecast = final_model.get_forecast_values(forecast_df)\n",
        "else:\n",
        "    # ML models\n",
        "    X_full = df_features[feature_cols]\n",
        "    if best_model_name == 'XGBoost':\n",
        "        final_model = XGBoostModel()\n",
        "        final_model.fit(X_full, y_full)\n",
        "        # For future predictions, we need to create features\n",
        "        # This is simplified - in production, you'd need to generate future features\n",
        "        final_forecast = final_model.predict(X_full.tail(config.FORECAST_HORIZON))\n",
        "    elif best_model_name == 'Random Forest':\n",
        "        final_model = RandomForestModel()\n",
        "        final_model.fit(X_full, y_full)\n",
        "        final_forecast = final_model.predict(X_full.tail(config.FORECAST_HORIZON))\n",
        "    else:\n",
        "        final_model = LinearRegressionModel()\n",
        "        final_model.fit(X_full, y_full)\n",
        "        final_forecast = final_model.predict(X_full.tail(config.FORECAST_HORIZON))\n",
        "\n",
        "# Create forecast index\n",
        "forecast_index = pd.date_range(\n",
        "    start=y_full.index[-1] + pd.Timedelta(hours=1),\n",
        "    periods=config.FORECAST_HORIZON,\n",
        "    freq='H'\n",
        ")\n",
        "\n",
        "# Plot final forecast\n",
        "plt.figure(figsize=(16, 6))\n",
        "plt.plot(y_full.index[-168:], y_full.values[-168:], label='Historical (last 7 days)', linewidth=2, alpha=0.7)\n",
        "plt.plot(forecast_index, final_forecast, label=f'Forecast ({best_model_name})', linewidth=2, linestyle='--', color='red')\n",
        "plt.axvline(x=y_full.index[-1], color='gray', linestyle=':', linewidth=2, label='Forecast Start')\n",
        "plt.xlabel('Date', fontsize=12)\n",
        "plt.ylabel('Global Active Power (kW)', fontsize=12)\n",
        "plt.title(f'{config.FORECAST_HORIZON}-Hour Ahead Forecast using {best_model_name}', fontsize=14, fontweight='bold')\n",
        "plt.legend(fontsize=11)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('outputs/final_forecast.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nâœ“ Forecast generated for {config.FORECAST_HORIZON} hours ahead\")\n",
        "print(f\"  Forecast range: {forecast_index[0]} to {forecast_index[-1]}\")\n",
        "print(f\"  Forecast values range: {final_forecast.min():.2f} to {final_forecast.max():.2f} kW\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate comprehensive evaluation report\n",
        "report = generate_evaluation_report(\n",
        "    all_metrics,\n",
        "    primary_metric='RMSE',\n",
        "    save_path='outputs/evaluation_report.txt'\n",
        ")\n",
        "print(report)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Key Findings and Conclusions\n",
        "\n",
        "### Summary\n",
        "\n",
        "1. **Data Quality**: Successfully handled 25,979 missing values and resampled to hourly frequency\n",
        "2. **Patterns Identified**: Strong daily (24h) and weekly (168h) seasonality patterns\n",
        "3. **Stationarity**: Series shows non-stationary behavior, requiring differencing for ARIMA models\n",
        "4. **Best Model**: [Will be determined after execution]\n",
        "\n",
        "### Recommendations\n",
        "\n",
        "1. **Model Selection**: Use the best performing model based on RMSE/MAPE on test set\n",
        "2. **Feature Engineering**: Lag features and rolling statistics significantly improve ML model performance\n",
        "3. **Monitoring**: Regularly retrain models as new data arrives\n",
        "4. **Ensemble**: Consider ensemble methods combining top-performing models\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "1. Deploy best model for production forecasting\n",
        "2. Set up monitoring and retraining pipeline\n",
        "3. Implement confidence intervals for uncertainty quantification\n",
        "4. Explore external features (weather, holidays) if available\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
